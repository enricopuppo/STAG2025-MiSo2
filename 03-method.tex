\section{Implementing elementary functions}
In order to implement robust, conservative and tight interval extensions of elementary functions in MiSo, we rely on existing \emph{correctly rounded} implementations of functions.
The CORE-MATH Project \cite{} provides a collection of fast, correctly rounded implementations of most commonly used mathematical functions.
For ease of use, we created a minimal C++ library of double-precision CORE-MATH implementations that is publicly available at \url{}.

The functions that we implemented are:
\begin{itemize}
	\item
\end{itemize}

\subsection{Interval extension}
Assuming the availability of a correctly rounded function $f$, we want to obtain a correctly rounded inclusion function for $f$, that is, an interval-valued function $\inclusion{f}$ such that the endpoints of the resulting interval are correctly rounded outward.

MiSo's interval class wraps the NFG interval library \cite{}, which guarantees that the result of every operation contains the true result, thanks to conservative outward rounding. Specifically, the library is initialized by setting the FPU rounding mode to upward, and the lower end of the interval is represented internally with opposite sign, which results in downward rounding without changing the rounding mode.

An issue that must be considered is how we deal with input intervals containing points outside the domain of $f$. We discuss this in Section \ref{}, and until then, assume that all inputs lie inside the domain of their functions.

There are two properties of a function that make it easier to be included in our framework: being monotonic, and being odd.

When $f$ is monotonically increasing on $[\intlo{x}, \inthi{x}]$, the image of the interval can be computed as $\inclusion{f}([\intlo{x}, \inthi{x}]) = [f(\intlo{x}), f(\inthi{x})]$; if it is monotonically decreasing, the two endpoints are swapped. In both cases, assuming exact arithmetic, the result is exact.
If $f$ is not monotonic on $[\intlo{x}, \inthi{x}]$, we need to know where $f$ attains its extrema on the interval. As we will see for some non-monotonic functions, even deciding that $[\intlo{x}, \inthi{x}]$ is in a monotonic region of the domain is tricky in rounded arithmetic.

Once one knows how to compute the range of the function in exact arithmetic, correct rounding amounts to rounding the left endpoint down, and the right endpoint up. Given that we are operating in round-upward mode, the latter is free. To round down we could change the rounding mode and reset it after the operation, but changing roundind modes is a very expensive operation since it flushes the CPU pipeline, and is best avoided.
When $f$ is odd, the result of $f(x)$ rounded down is easily computed in upward rounding mode as $-f(-x)$, since negation never causes error. Several elementary functions are odd, so this property is helpful for our purposes.

Therefore, for a monotonically increasing and odd function $f$, computing $\inclusion{f}$ with upward rounding is as easy as $\inclusion{f}([\intlo{x}, \inthi{x}]) = [-f(-\intlo{x}), f(\inthi{x})]$. This technique is used for the functions \FS{TODO}.

Next, we consider functions that are odd but not monotonic. These include $\tan$ and $\tan\pi$ \FS{TODO}. As explained, we need not take specific measures for rounding in this case: once we have determined where the extrema are, the right endpoint is computed normally, and the left is rounded down using the $f(x) = -f(-x)$ property. %elaborate more

If we drop the oddity assumption, we face the problem of how to round down the result of a call to the correctly-rounded library function without changing the rounding mode, since is a very expensive operation.
An easy way to deal with this is to compute the floating point number immediately below the rounded-up result.
However, if the exact result of the function is representable as a floating point number, correct rounding imposes that the exact result will be returned; in these special cases, if use the immediately smaller FP value, we get an error of 1 ULP.
While this is acceptable for practical use, it can create awkward situations for some known values: for example, computing $\inclusion{\sqrt{([0,x])}}$, where $x>0$, produces an interval with a \emph{negative} lower bound, instead of the expected value of 0.
To address this, we would like to only perform this operation if the upward rounding happened; unfortunately, the IEEE754 standard does not provide a way to do this, therefore we should check \emph{a priori} if the result will be exactly representable as a floating point number.

\FS{\dots}

\subsection{Points outside the domain}
