% !TEX root = STAG25-MiSo2.tex

\section{Background and state of the art}
\label{sec:related}

Interval arithmetic \cite{hickey2001} provides a set of operations on real intervals $\intervals$ such that if $x\in a=[\intlo{a},\inthi{a}] \in \intervals$ and $y\in b=[\intlo{b},\inthi{b}] \in \intervals$,
then $x \star y  \in a \star b$, where $\star$ in the right-hand side is the interval version of operation $\star$ on reals.
We are interested in intervals whose lower and upper bounds can be represented by FP numbers.

From now on, the set of representable FP numbers is denoted by $\mathbb{F}$. When $a$ and $b$ are in $\mathbb{F}$, the result $r = a \star b$ may be not in $\mathbb{F}$, and hence not representable. In that case $i=[fp^{-}(r),fp^{+}(r)]$, where $fp^{-}(r) = max(f : f \in \mathbb{F}, f \leq r)$ and $fp^{+}(r) = min(f : f \in \mathbb{F})$, is the tightest representable interval containing $r$.
IEEE 754 requires that when $\star$ is an algebraic operation (or the square root) an implementation of $\star$ must round the theoretically exact result $r$ to either $fp^{-}(r)$ or $fp^{+}(r)$, depending on the current \emph{rounding mode}.
In most modern architectures the rounding mode is controlled by a particular register within the CPU, and specific system functions exist to set it. Therefore, a trivial approach to create a tight interval for the operation $a \star b$ is to (1) set the rounding mode to $towards -\infty$, (2) execute $a \star b$ to determine the interval's lower bound, (3) set the rounding mode to $towards +\infty$, (4) execute $a \star b$ to determine the interval's upper bound.
Since setting the rounding mode is typically slower than executing arithmetic operations, a more efficient approach is (1) set the rounding mode to $towards +\infty$, (2) execute $(-a) \star b$ and switch the sign of the result to determine the interval's lower bound, (3) execute $a \star b$ to determine the interval's upper bound. Furthermore, if no other parts of the program require a different rounding, step (1) can be executed only once at the beginning.
This approach is used by existing interval arithmetic libraries such as Boost \cite{bronnimann2006} and CGAL \cite{cgal}.

Another possibility is to deconstruct the binary representation of the result $r$ to directly modify the mantissa, exponent and sign, and produce a reasonably small interval around $r$. This approach is used by Filib and Filib++ \cite{filib} \FS{Filib++ has multiple modes, is this true for all modes? should we be more specific?}.
Alternatively, the error propagation can be analyzed to derive a bound $\epsilon$ on the rounding so that the interval $i=[r-\epsilon,r+\epsilon]$ is guaranteed to contain $r$. This is how libraries such as BIAS \cite{bias} or GAOL \cite{gaol} work.

The aforementioned existing libraries were comprehensively compared by Tang and colleagues \cite{tang2022} who evaluated diverse aspects, including their correctness, efficiency and precision (in terms of interval tightness). Their conclusion is that only filib and filib++ are always correct when transcendental functions are involved, although the intervals they produce might be larger than necessary. In contrast, Boost and BIAS may produce intervals that do not contain the exact result. Also, Tang's evaluation could verify that libraries that use the rounding mode produce tighter intervals.

With the exception of rather old architectures, most existing CPUs provide SIMD registers and instructions that proved useful to accelerate interval arithmetic libraries \cite{lambov2008}. Here the basic idea is to store both the bounds in a single 128bit-wide register and perform operations on both bounds in parallel. This and other optimizations exploiting more recent AVX architectures were  included in the NFG library \cite{nfg} that, to the best of our knowledge, represents the fastest existing library at the time of writing. Since NFG exploits the rounding mode, it is also guaranteed to produce as-tight-as-possible intervals for all the algebraic operations and the square root. Our TIGHT library wraps around NFG while adding many other elementary and transcendental functions while keeping the guarantee to produce tight intervals.

\subsection{Correct rounding}
\emph{Correct rounding} refers to the property that an implementation
of a mathematical function $f$ has if, for any $x$ that is representable and contained in the domain of $f$, it returns the same results one would get by rounding the exact result $f(x)$ to the target representation.
While this property may seem obvious in theory, this is not what happens in real life: implementations of mathematical functions often consists of several steps, and the composition of two CR functions is not CR in general.
The IEEE754 standard for floating point arithmetic requires a compliant implementation of a function to round correctly for all inputs \cite{ieee}. Indeed, required operations such as summation, subtraction, multiplication, division, and square roots produce the same, correctly rounded results on any IEEE754-compliant machine.
However, this is not true for \emph{recommended} functions like $\sin$ or $\log$: because they are not mandatory, mathematical libraries are allowed to implement fast, non-CR routines that are not IEEE754-conforming, but the language implementation as a whole will be conforming as long as the mandatory operations are CR.
